{"dependencies":[{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1533242702575},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1533242702523},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1533242740549},{"name":"@tensorflow/tfjs-core","loc":{"line":11,"column":46}},{"name":"../activations","loc":{"line":12,"column":45}},{"name":"../backend/tfjs_backend","loc":{"line":14,"column":26}},{"name":"../engine/topology","loc":{"line":15,"column":22}},{"name":"../errors","loc":{"line":16,"column":36}},{"name":"../utils/generic_utils","loc":{"line":17,"column":31}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.Softmax = exports.ThresholdedReLU = exports.ELU = exports.LeakyReLU = undefined;\n\nvar _tfjsCore = require('@tensorflow/tfjs-core');\n\nvar _activations = require('../activations');\n\nvar _tfjs_backend = require('../backend/tfjs_backend');\n\nvar _topology = require('../engine/topology');\n\nvar _errors = require('../errors');\n\nvar _generic_utils = require('../utils/generic_utils');\n\nvar generic_utils = _interopRequireWildcard(_generic_utils);\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nvar __extends = undefined && undefined.__extends || function () {\n    var extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (d, b) {\n        d.__proto__ = b;\n    } || function (d, b) {\n        for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() {\n            this.constructor = d;\n        }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n}();\n\nvar LeakyReLU = function (_super) {\n    __extends(LeakyReLU, _super);\n    function LeakyReLU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_ALPHA = 0.3;\n        if (config == null) {\n            config = {};\n        }\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n        return _this;\n    }\n    LeakyReLU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return (0, _tfjsCore.leakyRelu)(x, this.alpha);\n    };\n    LeakyReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    LeakyReLU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    LeakyReLU.className = 'LeakyReLU';\n    return LeakyReLU;\n}(_topology.Layer);\nexports.LeakyReLU = LeakyReLU;\n\n_tfjsCore.serialization.SerializationMap.register(LeakyReLU);\nvar ELU = function (_super) {\n    __extends(ELU, _super);\n    function ELU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_ALPHA = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        if (config.alpha != null && config.alpha !== _this.DEFAULT_ALPHA) {\n            throw new _errors.NotImplementedError(\"Non-default alpha value (\" + config.alpha + \") is not supported by the \" + \"ELU layer yet.\");\n        }\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n        return _this;\n    }\n    ELU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return (0, _tfjsCore.elu)(x);\n    };\n    ELU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ELU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    ELU.className = 'ELU';\n    return ELU;\n}(_topology.Layer);\nexports.ELU = ELU;\n\n_tfjsCore.serialization.SerializationMap.register(ELU);\nvar ThresholdedReLU = function (_super) {\n    __extends(ThresholdedReLU, _super);\n    function ThresholdedReLU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_THETA = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        _this.theta = config.theta == null ? _this.DEFAULT_THETA : config.theta;\n        _this.thetaTensor = (0, _tfjs_backend.getScalar)(_this.theta);\n        return _this;\n    }\n    ThresholdedReLU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return x.mul((0, _tfjs_backend.cast)(x.greater(this.thetaTensor), 'float32'));\n    };\n    ThresholdedReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ThresholdedReLU.prototype.getConfig = function () {\n        var config = { theta: this.theta };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    ThresholdedReLU.className = 'ThresholdedReLU';\n    return ThresholdedReLU;\n}(_topology.Layer);\nexports.ThresholdedReLU = ThresholdedReLU;\n\n_tfjsCore.serialization.SerializationMap.register(ThresholdedReLU);\nvar Softmax = function (_super) {\n    __extends(Softmax, _super);\n    function Softmax(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_AXIS = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        _this.softmax = new _activations.Softmax().apply;\n        _this.axis = config.axis == null ? _this.DEFAULT_AXIS : config.axis;\n        return _this;\n    }\n    Softmax.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    };\n    Softmax.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    Softmax.prototype.getConfig = function () {\n        var config = { axis: this.axis };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    Softmax.className = 'Softmax';\n    return Softmax;\n}(_topology.Layer);\nexports.Softmax = Softmax;\n\n_tfjsCore.serialization.SerializationMap.register(Softmax);\n//# sourceMappingURL=advanced_activations.js.map"},"hash":"02b1c33e56c5fc2e6324ba5297a04348","cacheData":{"env":{}}}