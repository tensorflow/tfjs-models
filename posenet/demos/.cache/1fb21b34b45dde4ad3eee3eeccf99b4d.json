{"dependencies":[{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1533242702575},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1533242702523},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":1533242741391},{"name":"../environment","loc":{"line":11,"column":20}},{"name":"../globals","loc":{"line":12,"column":27}},{"name":"../ops/ops","loc":{"line":13,"column":34}},{"name":"../serialization","loc":{"line":14,"column":33}},{"name":"./optimizer","loc":{"line":15,"column":26}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.AdadeltaOptimizer = undefined;\n\nvar _environment = require('../environment');\n\nvar _globals = require('../globals');\n\nvar _ops = require('../ops/ops');\n\nvar _serialization = require('../serialization');\n\nvar _optimizer = require('./optimizer');\n\nvar __extends = undefined && undefined.__extends || function () {\n    var extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (d, b) {\n        d.__proto__ = b;\n    } || function (d, b) {\n        for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() {\n            this.constructor = d;\n        }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n}();\n\nvar AdadeltaOptimizer = function (_super) {\n    __extends(AdadeltaOptimizer, _super);\n    function AdadeltaOptimizer(learningRate, rho, epsilon) {\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        var _this = _super.call(this) || this;\n        _this.learningRate = learningRate;\n        _this.rho = rho;\n        _this.epsilon = epsilon;\n        _this.accumulatedGrads = {};\n        _this.accumulatedUpdates = {};\n        _this.c = (0, _globals.keep)((0, _ops.scalar)(-learningRate));\n        _this.epsilonScalar = (0, _globals.keep)((0, _ops.scalar)(epsilon));\n        _this.rhoScalar = (0, _globals.keep)((0, _ops.scalar)(rho));\n        _this.oneMinusRho = (0, _globals.keep)((0, _ops.scalar)(1 - rho));\n        return _this;\n    }\n    AdadeltaOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        var _loop_1 = function (variableName) {\n            var value = _environment.ENV.engine.registeredVariables[variableName];\n            if (this_1.accumulatedGrads[variableName] == null) {\n                var trainable_1 = false;\n                (0, _globals.tidy)(function () {\n                    _this.accumulatedGrads[variableName] = (0, _ops.zerosLike)(value).variable(trainable_1);\n                });\n            }\n            if (this_1.accumulatedUpdates[variableName] == null) {\n                var trainable_2 = false;\n                (0, _globals.tidy)(function () {\n                    _this.accumulatedUpdates[variableName] = (0, _ops.zerosLike)(value).variable(trainable_2);\n                });\n            }\n            var gradient = variableGradients[variableName];\n            var accumulatedGrad = this_1.accumulatedGrads[variableName];\n            var accumulatedUpdate = this_1.accumulatedUpdates[variableName];\n            (0, _globals.tidy)(function () {\n                var newAccumulatedGrad = _this.rhoScalar.mul(accumulatedGrad).add(_this.oneMinusRho.mul(gradient.square()));\n                var updates = accumulatedUpdate.add(_this.epsilonScalar).sqrt().div(accumulatedGrad.add(_this.epsilonScalar).sqrt()).mul(gradient);\n                var newAccumulatedUpdate = _this.rhoScalar.mul(accumulatedUpdate).add(_this.oneMinusRho.mul(updates.square()));\n                _this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\n                _this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\n                var newValue = _this.c.mul(updates).add(value);\n                value.assign(newValue);\n            });\n        };\n        var this_1 = this;\n        for (var variableName in variableGradients) {\n            _loop_1(variableName);\n        }\n    };\n    AdadeltaOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.epsilonScalar.dispose();\n        this.rhoScalar.dispose();\n        this.oneMinusRho.dispose();\n        if (this.accumulatedUpdates != null) {\n            Object.keys(this.accumulatedUpdates).forEach(function (name) {\n                return _this.accumulatedUpdates[name].dispose();\n            });\n            Object.keys(this.accumulatedGrads).forEach(function (name) {\n                return _this.accumulatedGrads[name].dispose();\n            });\n        }\n    };\n    AdadeltaOptimizer.prototype.getConfig = function () {\n        return {\n            learningRate: this.learningRate,\n            rho: this.rho,\n            epsilon: this.epsilon\n        };\n    };\n    AdadeltaOptimizer.fromConfig = function (cls, config) {\n        return new cls(config.learningRate, config.rho, config.epsilon);\n    };\n    AdadeltaOptimizer.className = 'AdadeltaOptimizer';\n    return AdadeltaOptimizer;\n}(_optimizer.Optimizer);\nexports.AdadeltaOptimizer = AdadeltaOptimizer;\n\n_serialization.SerializationMap.register(AdadeltaOptimizer);\n//# sourceMappingURL=adadelta_optimizer.js.map"},"hash":"a8a55b81e96585b197317e5c9b40d6d1","cacheData":{"env":{}}}