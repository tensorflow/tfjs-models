{"dependencies":[{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1533242702575},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1533242702523},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1533242740549},{"name":"@tensorflow/tfjs-core","loc":{"line":18,"column":47}},{"name":"../activations","loc":{"line":19,"column":51}},{"name":"../backend/tfjs_backend","loc":{"line":20,"column":19}},{"name":"../constraints","loc":{"line":21,"column":51}},{"name":"../engine/topology","loc":{"line":23,"column":22}},{"name":"../errors","loc":{"line":24,"column":64}},{"name":"../initializers","loc":{"line":25,"column":72}},{"name":"../regularizers","loc":{"line":26,"column":53}},{"name":"../types","loc":{"line":27,"column":31}},{"name":"../utils/generic_utils","loc":{"line":28,"column":31}},{"name":"../utils/math_utils","loc":{"line":29,"column":28}},{"name":"../variables","loc":{"line":30,"column":45}},{"name":"./serialization","loc":{"line":31,"column":28}}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.StackedRNNCells = exports.LSTM = exports.LSTMCell = exports.GRU = exports.GRUCell = exports.SimpleRNN = exports.SimpleRNNCell = exports.RNNCell = exports.RNN = undefined;\nexports.rnn = rnn;\n\nvar _tfjsCore = require(\"@tensorflow/tfjs-core\");\n\nvar tfc = _interopRequireWildcard(_tfjsCore);\n\nvar _activations = require(\"../activations\");\n\nvar _tfjs_backend = require(\"../backend/tfjs_backend\");\n\nvar K = _interopRequireWildcard(_tfjs_backend);\n\nvar _constraints = require(\"../constraints\");\n\nvar _topology = require(\"../engine/topology\");\n\nvar _errors = require(\"../errors\");\n\nvar _initializers = require(\"../initializers\");\n\nvar _regularizers = require(\"../regularizers\");\n\nvar _types = require(\"../types\");\n\nvar _generic_utils = require(\"../utils/generic_utils\");\n\nvar generic_utils = _interopRequireWildcard(_generic_utils);\n\nvar _math_utils = require(\"../utils/math_utils\");\n\nvar math_utils = _interopRequireWildcard(_math_utils);\n\nvar _variables = require(\"../variables\");\n\nvar _serialization = require(\"./serialization\");\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nvar __extends = undefined && undefined.__extends || function () {\n    var extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (d, b) {\n        d.__proto__ = b;\n    } || function (d, b) {\n        for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() {\n            this.constructor = d;\n        }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n}();\nvar __decorate = undefined && undefined.__decorate || function (decorators, target, key, desc) {\n    var c = arguments.length,\n        r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n        d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nfunction rnn(stepFunction, inputs, initialStates, goBackwards, mask, constants, unroll, inputLength) {\n    if (goBackwards === void 0) {\n        goBackwards = false;\n    }\n    if (unroll === void 0) {\n        unroll = false;\n    }\n    var ndim = inputs.shape.length;\n    if (ndim < 3) {\n        throw new _errors.ValueError(\"Input should be at least 3D, but is \" + ndim + \"D.\");\n    }\n    var axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n    if (mask != null) {\n        throw new _errors.NotImplementedError('The rnn() function of the deeplearn.js backend does not support ' + 'masking yet.');\n    }\n    if (constants != null) {\n        throw new _errors.NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    }\n    if (unroll) {\n        console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n    if (goBackwards) {\n        inputs = tfc.reverse(inputs, 0);\n    }\n    var outputs;\n    var lastOutput;\n    var states = initialStates;\n    var timeSteps = inputs.shape[0];\n    for (var t = 0; t < timeSteps; ++t) {\n        var currentInput = K.sliceAlongFirstAxis(inputs, t, 1);\n        currentInput = currentInput.reshape(currentInput.shape.slice(1));\n        var stepOutputs = stepFunction(currentInput, states);\n        lastOutput = stepOutputs[0];\n        if (t === 0) {\n            outputs = lastOutput.reshape([1].concat(lastOutput.shape));\n        } else {\n            outputs = K.concatAlongFirstAxis(outputs, lastOutput.reshape([1].concat(lastOutput.shape)));\n        }\n        states = stepOutputs[1];\n    }\n    return [lastOutput, tfc.transpose(outputs, [1, 0].concat(math_utils.range(2, outputs.shape.length))), states];\n}\nvar RNN = function (_super) {\n    __extends(RNN, _super);\n    function RNN(config) {\n        var _this = _super.call(this, config) || this;\n        var cell;\n        if (config.cell == null) {\n            throw new _errors.ValueError('cell property is missing for the constructor of RNN.');\n        } else if (Array.isArray(config.cell)) {\n            cell = new StackedRNNCells({ cells: config.cell });\n        } else {\n            cell = config.cell;\n        }\n        if (cell.stateSize == null) {\n            throw new _errors.ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n        }\n        _this.cell = cell;\n        _this.returnSequences = config.returnSequences == null ? false : config.returnSequences;\n        _this.returnState = config.returnState == null ? false : config.returnState;\n        _this.goBackwards = config.goBackwards == null ? false : config.goBackwards;\n        _this._stateful = config.stateful == null ? false : config.stateful;\n        _this.unroll = config.unroll == null ? false : config.unroll;\n        _this.supportsMasking = true;\n        _this.inputSpec = [new _topology.InputSpec({ ndim: 3 })];\n        _this.stateSpec = null;\n        _this.states = null;\n        _this.numConstants = null;\n        return _this;\n    }\n    RNN.prototype.getStates = function () {\n        if (this.states == null) {\n            var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n            return math_utils.range(0, numStates).map(function (x) {\n                return null;\n            });\n        } else {\n            return this.states;\n        }\n    };\n    RNN.prototype.setStates = function (states) {\n        this.states = states;\n    };\n    RNN.prototype.computeOutputShape = function (inputShape) {\n        if (generic_utils.isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        var stateSize = this.cell.stateSize;\n        if (!Array.isArray(stateSize)) {\n            stateSize = [stateSize];\n        }\n        var outputDim = stateSize[0];\n        var outputShape;\n        if (this.returnSequences) {\n            outputShape = [inputShape[0], inputShape[1], outputDim];\n        } else {\n            outputShape = [inputShape[0], outputDim];\n        }\n        if (this.returnState) {\n            var stateShape = [];\n            for (var _i = 0, stateSize_1 = stateSize; _i < stateSize_1.length; _i++) {\n                var dim = stateSize_1[_i];\n                stateShape.push([inputShape[0], dim]);\n            }\n            return [outputShape].concat(stateShape);\n        } else {\n            return outputShape;\n        }\n    };\n    RNN.prototype.computeMask = function (inputs, mask) {\n        throw new _errors.NotImplementedError('computeMask has not been implemented for RNN yet');\n    };\n    RNN.prototype.build = function (inputShape) {\n        var constantShape = null;\n        if (this.numConstants != null) {\n            throw new _errors.NotImplementedError('Constants support is not implemented in RNN yet.');\n        }\n        if (generic_utils.isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        var batchSize = this.stateful ? inputShape[0] : null;\n        var inputDim = inputShape[inputShape.length - 1];\n        this.inputSpec[0] = new _topology.InputSpec({ shape: [batchSize, null, inputDim] });\n        var stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (constantShape != null) {\n            throw new _errors.NotImplementedError('Constants support is not implemented in RNN yet.');\n        } else {\n            this.cell.build(stepInputShape);\n        }\n        var stateSize;\n        if (Array.isArray(this.cell.stateSize)) {\n            stateSize = this.cell.stateSize;\n        } else {\n            stateSize = [this.cell.stateSize];\n        }\n        if (this.stateSpec != null) {\n            if (!_tfjsCore.util.arraysEqual(this.stateSpec.map(function (spec) {\n                return spec.shape[spec.shape.length - 1];\n            }), stateSize)) {\n                throw new _errors.ValueError(\"An initialState was passed that is not compatible with \" + (\"cell.stateSize. Received stateSpec=\" + this.stateSpec + \"; \") + (\"However cell.stateSize is \" + this.cell.stateSize));\n            }\n        } else {\n            this.stateSpec = stateSize.map(function (dim) {\n                return new _topology.InputSpec({ shape: [null, dim] });\n            });\n        }\n        if (this.stateful) {\n            throw new _errors.NotImplementedError('stateful RNN layer is not implemented yet');\n        }\n    };\n    RNN.prototype.resetStates = function (states) {\n        var _this = this;\n        (0, _tfjsCore.tidy)(function () {\n            if (!_this.stateful) {\n                throw new _errors.AttributeError('Cannot call resetState() on an RNN Layer that is not stateful.');\n            }\n            var batchSize = _this.inputSpec[0].shape[0];\n            if (batchSize == null) {\n                throw new _errors.ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n            }\n            if (_this.states == null) {\n                if (Array.isArray(_this.cell.stateSize)) {\n                    _this.states = _this.cell.stateSize.map(function (dim) {\n                        return tfc.zeros([batchSize, dim]);\n                    });\n                } else {\n                    _this.states = [tfc.zeros([batchSize, _this.cell.stateSize])];\n                }\n            } else if (states == null) {\n                if (Array.isArray(_this.cell.stateSize)) {\n                    _this.states = _this.cell.stateSize.map(function (dim) {\n                        return tfc.zeros([batchSize, dim]);\n                    });\n                } else {\n                    _this.states[0] = tfc.zeros([batchSize, _this.cell.stateSize]);\n                }\n            } else {\n                if (!Array.isArray(states)) {\n                    states = [states];\n                }\n                if (states.length !== _this.states.length) {\n                    throw new _errors.ValueError(\"Layer \" + _this.name + \" expects \" + _this.states.length + \" state(s), \" + (\"but it received \" + states.length + \" state value(s). Input \") + (\"received: \" + states));\n                }\n                for (var index = 0; index < _this.states.length; ++index) {\n                    var value = states[index];\n                    var dim = Array.isArray(_this.cell.stateSize) ? _this.cell.stateSize[index] : _this.cell.stateSize;\n                    var expectedShape = [batchSize, dim];\n                    if (!_tfjsCore.util.arraysEqual(value.shape, expectedShape)) {\n                        throw new _errors.ValueError(\"State \" + index + \" is incompatible with layer \" + _this.name + \": \" + (\"expected shape=\" + expectedShape + \", received shape=\" + value.shape));\n                    }\n                    _this.states[index] = value;\n                }\n            }\n        });\n    };\n    RNN.prototype.standardizeArgs = function (inputs, initialState, constants) {\n        if (Array.isArray(inputs)) {\n            if (initialState != null || constants != null) {\n                throw new _errors.ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n            }\n            if (this.numConstants != null) {\n                constants = inputs.slice(inputs.length - this.numConstants, inputs.length);\n                inputs = inputs.slice(0, inputs.length - this.numConstants);\n            }\n            if (inputs.length > 1) {\n                initialState = inputs.slice(1, inputs.length);\n            }\n            inputs = inputs[0];\n        }\n        function toListOrNull(x) {\n            if (x == null || Array.isArray(x)) {\n                return x;\n            } else {\n                return [x];\n            }\n        }\n        initialState = toListOrNull(initialState);\n        constants = toListOrNull(constants);\n        return { inputs: inputs, initialState: initialState, constants: constants };\n    };\n    RNN.prototype.apply = function (inputs, kwargs) {\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        var constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        var standardized = this.standardizeArgs(inputs, initialState, constants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        var additionalInputs = [];\n        var additionalSpecs = [];\n        if (initialState != null) {\n            kwargs['initialState'] = initialState;\n            additionalInputs = additionalInputs.concat(initialState);\n            this.stateSpec = [];\n            for (var _i = 0, initialState_1 = initialState; _i < initialState_1.length; _i++) {\n                var state = initialState_1[_i];\n                this.stateSpec.push(new _topology.InputSpec({ shape: state.shape }));\n            }\n            additionalSpecs = additionalSpecs.concat(this.stateSpec);\n        }\n        if (constants != null) {\n            kwargs['constants'] = constants;\n            additionalInputs = additionalInputs.concat(constants);\n            this.numConstants = constants.length;\n        }\n        var isTensor = additionalInputs[0] instanceof _types.SymbolicTensor;\n        if (isTensor) {\n            var fullInput = [inputs].concat(additionalInputs);\n            var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            var originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            var output = _super.prototype.apply.call(this, fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        } else {\n            return _super.prototype.apply.call(this, inputs, kwargs);\n        }\n    };\n    RNN.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            var mask = kwargs == null ? null : kwargs['mask'];\n            var training = kwargs == null ? null : kwargs['training'];\n            var initialState = kwargs == null ? null : kwargs['initialState'];\n            inputs = generic_utils.getExactlyOneTensor(inputs);\n            if (initialState == null) {\n                if (_this.stateful) {\n                    throw new _errors.NotImplementedError('stateful RNN layer is not implemented yet.');\n                } else {\n                    initialState = _this.getInitialState(inputs);\n                }\n            }\n            if (mask != null) {\n                throw new _errors.NotImplementedError('Masking is not implemented for RNN yet');\n            }\n            var numStates = Array.isArray(_this.cell.stateSize) ? _this.cell.stateSize.length : 1;\n            if (initialState.length !== numStates) {\n                throw new _errors.ValueError(\"RNN Layer has \" + numStates + \" state(s) but was passed \" + (initialState.length + \" initial state(s).\"));\n            }\n            var inputShape = inputs.shape;\n            var timesteps = inputShape[1];\n            if (_this.unroll) {\n                console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n            }\n            var cellCallKwargs = { training: training };\n            var step = function (inputs, states) {\n                var outputs = _this.cell.call([inputs].concat(states), cellCallKwargs);\n                return [outputs[0], outputs.slice(1)];\n            };\n            var rnnOutputs = rnn(step, inputs, initialState, _this.goBackwards, null, null, _this.unroll, timesteps);\n            var lastOutput = rnnOutputs[0];\n            var outputs = rnnOutputs[1];\n            var states = rnnOutputs[2];\n            if (_this.stateful) {\n                throw new _errors.NotImplementedError('stateful RNN layer is not implemented yet');\n            }\n            var output = _this.returnSequences ? outputs : lastOutput;\n            if (_this.returnState) {\n                return [output].concat(states);\n            } else {\n                return output;\n            }\n        });\n    };\n    RNN.prototype.getInitialState = function (inputs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            var initialState = tfc.zeros(inputs.shape);\n            initialState = tfc.sum(initialState, [1, 2]);\n            initialState = K.expandDims(initialState);\n            if (Array.isArray(_this.cell.stateSize)) {\n                return _this.cell.stateSize.map(function (dim) {\n                    return dim > 1 ? K.tile(initialState, [1, dim]) : initialState;\n                });\n            } else {\n                return _this.cell.stateSize > 1 ? [K.tile(initialState, [1, _this.cell.stateSize])] : [initialState];\n            }\n        });\n    };\n    Object.defineProperty(RNN.prototype, \"trainableWeights\", {\n        get: function () {\n            if (!this.trainable) {\n                return [];\n            }\n            return this.cell.trainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(RNN.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            if (!this.trainable) {\n                return this.cell.weights;\n            }\n            return this.cell.nonTrainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    RNN.prototype.getConfig = function () {\n        var config = {\n            returnSequences: this.returnSequences,\n            returnState: this.returnState,\n            goBackwards: this.goBackwards,\n            stateful: this.stateful,\n            unroll: this.unroll\n        };\n        if (this.numConstants != null) {\n            config.numConstants = this.numConstants;\n        }\n        var cellConfig = this.cell.getConfig();\n        config.cell = {\n            className: this.cell.getClassName(),\n            config: cellConfig\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    RNN.className = 'RNN';\n    return RNN;\n}(_topology.Layer);\nexports.RNN = RNN;\n\n_tfjsCore.serialization.SerializationMap.register(RNN);\nvar RNNCell = function (_super) {\n    __extends(RNNCell, _super);\n    function RNNCell() {\n        return _super !== null && _super.apply(this, arguments) || this;\n    }\n    RNNCell = __decorate([(0, _tfjsCore.doc)({ heading: 'Layers', subheading: 'Classes' })], RNNCell);\n    return RNNCell;\n}(_topology.Layer);\nexports.RNNCell = RNNCell;\n\nvar SimpleRNNCell = function (_super) {\n    __extends(SimpleRNNCell, _super);\n    function SimpleRNNCell(config) {\n        var _this = _super.call(this, config) || this;\n        _this.DEFAULT_ACTIVATION = 'tanh';\n        _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        _this.units = config.units;\n        _this.activation = (0, _activations.getActivation)(config.activation == null ? _this.DEFAULT_ACTIVATION : config.activation);\n        _this.useBias = config.useBias == null ? true : config.useBias;\n        _this.kernelInitializer = (0, _initializers.getInitializer)(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n        _this.recurrentInitializer = (0, _initializers.getInitializer)(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n        _this.biasInitializer = (0, _initializers.getInitializer)(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n        _this.kernelRegularizer = (0, _regularizers.getRegularizer)(config.kernelRegularizer);\n        _this.recurrentRegularizer = (0, _regularizers.getRegularizer)(config.recurrentRegularizer);\n        _this.biasRegularizer = (0, _regularizers.getRegularizer)(config.biasRegularizer);\n        _this.kernelConstraint = (0, _constraints.getConstraint)(config.kernelConstraint);\n        _this.recurrentConstraint = (0, _constraints.getConstraint)(config.recurrentConstraint);\n        _this.biasConstraint = (0, _constraints.getConstraint)(config.biasConstraint);\n        _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n        _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n        _this.stateSize = _this.units;\n        return _this;\n    }\n    SimpleRNNCell.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        } else {\n            this.bias = null;\n        }\n        this.built = true;\n    };\n    SimpleRNNCell.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new _errors.ValueError(\"SimpleRNNCell expects 2 input Tensors, got \" + inputs.length + \".\");\n            }\n            var prevOutput = inputs[1];\n            inputs = inputs[0];\n            if (_this.dropout !== 0 || _this.recurrentDropout !== 0) {\n                throw new _errors.NotImplementedError('Dropout is not implemented for SimpleRNNCell yet');\n            }\n            var h = K.dot(inputs, _this.kernel.read());\n            if (_this.bias != null) {\n                h = K.biasAdd(h, _this.bias.read());\n            }\n            var output = tfc.add(h, K.dot(prevOutput, _this.recurrentKernel.read()));\n            if (_this.activation != null) {\n                output = _this.activation.apply(output);\n            }\n            return [output, output];\n        });\n    };\n    SimpleRNNCell.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    SimpleRNNCell.className = 'SimpleRNNCell';\n    return SimpleRNNCell;\n}(RNNCell);\nexports.SimpleRNNCell = SimpleRNNCell;\n\n_tfjsCore.serialization.SerializationMap.register(SimpleRNNCell);\nvar SimpleRNN = function (_super) {\n    __extends(SimpleRNN, _super);\n    function SimpleRNN(config) {\n        var _this = this;\n        config.cell = new SimpleRNNCell(config);\n        _this = _super.call(this, config) || this;\n        return _this;\n    }\n    SimpleRNN.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            var mask = kwargs == null ? null : kwargs['mask'];\n            var training = kwargs == null ? null : kwargs['training'];\n            var initialState = kwargs == null ? null : kwargs['initialState'];\n            return _super.prototype.call.call(_this, inputs, { mask: mask, training: training, initialState: initialState });\n        });\n    };\n    Object.defineProperty(SimpleRNN.prototype, \"units\", {\n        get: function () {\n            return this.cell.units;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"activation\", {\n        get: function () {\n            return this.cell.activation;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"useBias\", {\n        get: function () {\n            return this.cell.useBias;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"kernelInitializer\", {\n        get: function () {\n            return this.cell.kernelInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"recurrentInitializer\", {\n        get: function () {\n            return this.cell.recurrentInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"biasInitializer\", {\n        get: function () {\n            return this.cell.biasInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"kernelRegularizer\", {\n        get: function () {\n            return this.cell.kernelRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"recurrentRegularizer\", {\n        get: function () {\n            return this.cell.recurrentRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"biasRegularizer\", {\n        get: function () {\n            return this.cell.biasRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"kernelConstraint\", {\n        get: function () {\n            return this.cell.kernelConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"recurrentConstraint\", {\n        get: function () {\n            return this.cell.recurrentConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"biasConstraint\", {\n        get: function () {\n            return this.cell.biasConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"dropout\", {\n        get: function () {\n            return this.cell.dropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(SimpleRNN.prototype, \"recurrentDropout\", {\n        get: function () {\n            return this.cell.recurrentDropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    SimpleRNN.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        delete baseConfig['cell'];\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    SimpleRNN.className = 'SimpleRNN';\n    return SimpleRNN;\n}(RNN);\nexports.SimpleRNN = SimpleRNN;\n\n_tfjsCore.serialization.SerializationMap.register(SimpleRNN);\nvar GRUCell = function (_super) {\n    __extends(GRUCell, _super);\n    function GRUCell(config) {\n        var _this = _super.call(this, config) || this;\n        _this.DEFAULT_ACTIVATION = 'tanh';\n        _this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        _this.units = config.units;\n        _this.activation = (0, _activations.getActivation)(config.activation === undefined ? _this.DEFAULT_ACTIVATION : config.activation);\n        _this.recurrentActivation = (0, _activations.getActivation)(config.activation === undefined ? _this.DEFAULT_RECURRENT_ACTIVATION : config.recurrentActivation);\n        _this.useBias = config.useBias == null ? true : config.useBias;\n        _this.kernelInitializer = (0, _initializers.getInitializer)(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n        _this.recurrentInitializer = (0, _initializers.getInitializer)(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n        _this.biasInitializer = (0, _initializers.getInitializer)(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n        _this.kernelRegularizer = (0, _regularizers.getRegularizer)(config.kernelRegularizer);\n        _this.recurrentRegularizer = (0, _regularizers.getRegularizer)(config.recurrentRegularizer);\n        _this.biasRegularizer = (0, _regularizers.getRegularizer)(config.biasRegularizer);\n        _this.kernelConstraint = (0, _constraints.getConstraint)(config.kernelConstraint);\n        _this.recurrentConstraint = (0, _constraints.getConstraint)(config.recurrentConstraint);\n        _this.biasConstraint = (0, _constraints.getConstraint)(config.biasConstraint);\n        _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n        _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n        _this.implementation = config.implementation;\n        _this.stateSize = _this.units;\n        return _this;\n    }\n    GRUCell.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        var inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        if (this.useBias) {\n            this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        } else {\n            this.bias = null;\n        }\n        this.built = true;\n    };\n    GRUCell.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            if (_this.dropout !== 0 || _this.recurrentDropout !== 0) {\n                throw new _errors.NotImplementedError('Dropout is not implemented for GRUCell yet');\n            }\n            inputs = inputs;\n            if (inputs.length !== 2) {\n                throw new _errors.ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + (inputs.length + \".\"));\n            }\n            var hTMinus1 = inputs[1];\n            inputs = inputs[0];\n            var z;\n            var r;\n            var hh;\n            if (_this.implementation === 1) {\n                var kernelZ = K.sliceAlongLastAxis(_this.kernel.read(), 0, _this.units);\n                var kernelR = K.sliceAlongLastAxis(_this.kernel.read(), _this.units, _this.units);\n                var kernelH = K.sliceAlongLastAxis(_this.kernel.read(), _this.units * 2, _this.units);\n                var recurrentKernelZ = K.sliceAlongLastAxis(_this.recurrentKernel.read(), 0, _this.units);\n                var recurrentKernelR = K.sliceAlongLastAxis(_this.recurrentKernel.read(), _this.units, _this.units);\n                var recurrentKernelH = K.sliceAlongLastAxis(_this.recurrentKernel.read(), _this.units * 2, _this.units);\n                var inputsZ = inputs;\n                var inputsR = inputs;\n                var inputsH = inputs;\n                var xZ = K.dot(inputsZ, kernelZ);\n                var xR = K.dot(inputsR, kernelR);\n                var xH = K.dot(inputsH, kernelH);\n                if (_this.useBias) {\n                    var biasZ = K.sliceAlongFirstAxis(_this.bias.read(), 0, _this.units);\n                    var biasR = K.sliceAlongFirstAxis(_this.bias.read(), _this.units, _this.units);\n                    var biasH = K.sliceAlongFirstAxis(_this.bias.read(), _this.units * 2, _this.units);\n                    xZ = K.biasAdd(xZ, biasZ);\n                    xR = K.biasAdd(xR, biasR);\n                    xH = K.biasAdd(xH, biasH);\n                }\n                var hTMinus1Z = hTMinus1;\n                var hTMinus1R = hTMinus1;\n                var hTMinus1H = hTMinus1;\n                z = _this.recurrentActivation.apply(tfc.add(xZ, K.dot(hTMinus1Z, recurrentKernelZ)));\n                r = _this.recurrentActivation.apply(tfc.add(xR, K.dot(hTMinus1R, recurrentKernelR)));\n                hh = _this.activation.apply(tfc.add(xH, K.dot(tfc.mul(r, hTMinus1H), recurrentKernelH)));\n            } else {\n                var matrixX = K.dot(inputs, _this.kernel.read());\n                if (_this.useBias) {\n                    matrixX = K.biasAdd(matrixX, _this.bias.read());\n                }\n                var matrixInner = K.dot(hTMinus1, K.sliceAlongLastAxis(_this.recurrentKernel.read(), 0, 2 * _this.units));\n                var xZ = K.sliceAlongLastAxis(matrixX, 0, _this.units);\n                var xR = K.sliceAlongLastAxis(matrixX, _this.units, _this.units);\n                var recurrentZ = K.sliceAlongLastAxis(matrixInner, 0, _this.units);\n                var recurrentR = K.sliceAlongLastAxis(matrixInner, _this.units, _this.units);\n                z = _this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n                r = _this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n                var xH = K.sliceAlongLastAxis(matrixX, 2 * _this.units, _this.units);\n                var recurrentH = K.dot(tfc.mul(r, hTMinus1), K.sliceAlongLastAxis(_this.recurrentKernel.read(), 2 * _this.units, _this.units));\n                hh = _this.activation.apply(tfc.add(xH, recurrentH));\n            }\n            var h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(K.scalarPlusArray(K.getScalar(1), tfc.neg(z)), hh));\n            return [h, h];\n        });\n    };\n    GRUCell.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    GRUCell.className = 'GRUCell';\n    return GRUCell;\n}(RNNCell);\nexports.GRUCell = GRUCell;\n\n_tfjsCore.serialization.SerializationMap.register(GRUCell);\nvar GRU = function (_super) {\n    __extends(GRU, _super);\n    function GRU(config) {\n        var _this = this;\n        if (config.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n        }\n        config.cell = new GRUCell(config);\n        _this = _super.call(this, config) || this;\n        return _this;\n    }\n    GRU.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            var mask = kwargs == null ? null : kwargs['mask'];\n            var training = kwargs == null ? null : kwargs['training'];\n            var initialState = kwargs == null ? null : kwargs['initialState'];\n            return _super.prototype.call.call(_this, inputs, { mask: mask, training: training, initialState: initialState });\n        });\n    };\n    Object.defineProperty(GRU.prototype, \"units\", {\n        get: function () {\n            return this.cell.units;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"activation\", {\n        get: function () {\n            return this.cell.activation;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"useBias\", {\n        get: function () {\n            return this.cell.useBias;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"kernelInitializer\", {\n        get: function () {\n            return this.cell.kernelInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"recurrentInitializer\", {\n        get: function () {\n            return this.cell.recurrentInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"biasInitializer\", {\n        get: function () {\n            return this.cell.biasInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"kernelRegularizer\", {\n        get: function () {\n            return this.cell.kernelRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"recurrentRegularizer\", {\n        get: function () {\n            return this.cell.recurrentRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"biasRegularizer\", {\n        get: function () {\n            return this.cell.biasRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"kernelConstraint\", {\n        get: function () {\n            return this.cell.kernelConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"recurrentConstraint\", {\n        get: function () {\n            return this.cell.recurrentConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"biasConstraint\", {\n        get: function () {\n            return this.cell.biasConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"dropout\", {\n        get: function () {\n            return this.cell.dropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"recurrentDropout\", {\n        get: function () {\n            return this.cell.recurrentDropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(GRU.prototype, \"implementation\", {\n        get: function () {\n            return this.cell.implementation;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    GRU.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        delete baseConfig['cell'];\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    GRU.fromConfig = function (cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    };\n    GRU.className = 'GRU';\n    return GRU;\n}(RNN);\nexports.GRU = GRU;\n\n_tfjsCore.serialization.SerializationMap.register(GRU);\nvar LSTMCell = function (_super) {\n    __extends(LSTMCell, _super);\n    function LSTMCell(config) {\n        var _this = _super.call(this, config) || this;\n        _this.DEFAULT_ACTIVATION = 'tanh';\n        _this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n        _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n        _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n        _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n        _this.units = config.units;\n        _this.activation = (0, _activations.getActivation)(config.activation === undefined ? _this.DEFAULT_ACTIVATION : config.activation);\n        _this.recurrentActivation = (0, _activations.getActivation)(config.activation === undefined ? _this.DEFAULT_RECURRENT_ACTIVATION : config.recurrentActivation);\n        _this.useBias = config.useBias == null ? true : config.useBias;\n        _this.kernelInitializer = (0, _initializers.getInitializer)(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n        _this.recurrentInitializer = (0, _initializers.getInitializer)(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n        _this.biasInitializer = (0, _initializers.getInitializer)(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n        _this.unitForgetBias = config.unitForgetBias;\n        _this.kernelRegularizer = (0, _regularizers.getRegularizer)(config.kernelRegularizer);\n        _this.recurrentRegularizer = (0, _regularizers.getRegularizer)(config.recurrentRegularizer);\n        _this.biasRegularizer = (0, _regularizers.getRegularizer)(config.biasRegularizer);\n        _this.kernelConstraint = (0, _constraints.getConstraint)(config.kernelConstraint);\n        _this.recurrentConstraint = (0, _constraints.getConstraint)(config.recurrentConstraint);\n        _this.biasConstraint = (0, _constraints.getConstraint)(config.biasConstraint);\n        _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n        _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n        _this.implementation = config.implementation;\n        _this.stateSize = [_this.units, _this.units];\n        return _this;\n    }\n    LSTMCell.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        var inputDim = inputShape[inputShape.length - 1];\n        this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n        this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n        var biasInitializer;\n        if (this.useBias) {\n            if (this.unitForgetBias) {\n                var capturedBiasInit_1 = this.biasInitializer;\n                var capturedUnits_1 = this.units;\n                biasInitializer = new (_a = function (_super) {\n                    __extends(CustomInit, _super);\n                    function CustomInit() {\n                        return _super !== null && _super.apply(this, arguments) || this;\n                    }\n                    CustomInit.prototype.apply = function (shape, dtype) {\n                        var bI = capturedBiasInit_1.apply([capturedUnits_1]);\n                        var bF = new _initializers.Ones().apply([capturedUnits_1]);\n                        var bCAndH = capturedBiasInit_1.apply([capturedUnits_1 * 2]);\n                        return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n                    };\n                    return CustomInit;\n                }(_initializers.Initializer), _a.className = 'CustomInit', _a)();\n            } else {\n                biasInitializer = this.biasInitializer;\n            }\n            this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n        } else {\n            this.bias = null;\n        }\n        this.built = true;\n        var _a;\n    };\n    LSTMCell.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            if (_this.dropout !== 0 || _this.recurrentDropout !== 0) {\n                throw new _errors.NotImplementedError('Dropout is not implemented for LSTMCell yet');\n            }\n            inputs = inputs;\n            if (inputs.length !== 3) {\n                throw new _errors.ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + (inputs.length + \".\"));\n            }\n            var hTMinus1 = inputs[1];\n            var cTMinus1 = inputs[2];\n            inputs = inputs[0];\n            var i;\n            var f;\n            var c;\n            var o;\n            if (_this.implementation === 1) {\n                var kernelI = K.sliceAlongLastAxis(_this.kernel.read(), 0, _this.units);\n                var kernelF = K.sliceAlongLastAxis(_this.kernel.read(), _this.units, _this.units);\n                var kernelC = K.sliceAlongLastAxis(_this.kernel.read(), _this.units * 2, _this.units);\n                var kernelO = K.sliceAlongLastAxis(_this.kernel.read(), _this.units * 3, _this.units);\n                var recurrentKernelI = K.sliceAlongLastAxis(_this.recurrentKernel.read(), 0, _this.units);\n                var recurrentKernelF = K.sliceAlongLastAxis(_this.recurrentKernel.read(), _this.units, _this.units);\n                var recurrentKernelC = K.sliceAlongLastAxis(_this.recurrentKernel.read(), _this.units * 2, _this.units);\n                var recurrentKernelO = K.sliceAlongLastAxis(_this.recurrentKernel.read(), _this.units * 3, _this.units);\n                var inputsI = inputs;\n                var inputsF = inputs;\n                var inputsC = inputs;\n                var inputsO = inputs;\n                var xI = K.dot(inputsI, kernelI);\n                var xF = K.dot(inputsF, kernelF);\n                var xC = K.dot(inputsC, kernelC);\n                var xO = K.dot(inputsO, kernelO);\n                if (_this.useBias) {\n                    var biasI = K.sliceAlongFirstAxis(_this.bias.read(), 0, _this.units);\n                    var biasF = K.sliceAlongFirstAxis(_this.bias.read(), _this.units, _this.units);\n                    var biasC = K.sliceAlongFirstAxis(_this.bias.read(), _this.units * 2, _this.units);\n                    var biasO = K.sliceAlongFirstAxis(_this.bias.read(), _this.units * 3, _this.units);\n                    xI = K.biasAdd(xI, biasI);\n                    xF = K.biasAdd(xF, biasF);\n                    xC = K.biasAdd(xC, biasC);\n                    xO = K.biasAdd(xO, biasO);\n                }\n                var hTMinus1I = hTMinus1;\n                var hTMinus1F = hTMinus1;\n                var hTMinus1C = hTMinus1;\n                var hTMinus1O = hTMinus1;\n                i = _this.recurrentActivation.apply(tfc.add(xI, K.dot(hTMinus1I, recurrentKernelI)));\n                f = _this.recurrentActivation.apply(tfc.add(xF, K.dot(hTMinus1F, recurrentKernelF)));\n                c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, _this.activation.apply(tfc.add(xC, K.dot(hTMinus1C, recurrentKernelC)))));\n                o = _this.recurrentActivation.apply(tfc.add(xO, K.dot(hTMinus1O, recurrentKernelO)));\n            } else {\n                var z = K.dot(inputs, _this.kernel.read());\n                z = tfc.add(z, K.dot(hTMinus1, _this.recurrentKernel.read()));\n                if (_this.useBias) {\n                    z = K.biasAdd(z, _this.bias.read());\n                }\n                var z0 = K.sliceAlongLastAxis(z, 0, _this.units);\n                var z1 = K.sliceAlongLastAxis(z, _this.units, _this.units);\n                var z2 = K.sliceAlongLastAxis(z, _this.units * 2, _this.units);\n                var z3 = K.sliceAlongLastAxis(z, _this.units * 3, _this.units);\n                i = _this.recurrentActivation.apply(z0);\n                f = _this.recurrentActivation.apply(z1);\n                c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, _this.activation.apply(z2)));\n                o = _this.recurrentActivation.apply(z3);\n            }\n            var h = tfc.mul(o, _this.activation.apply(c));\n            return [h, h, c];\n        });\n    };\n    LSTMCell.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            unitForgetBias: this.unitForgetBias,\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    LSTMCell.className = 'LSTMCell';\n    return LSTMCell;\n}(RNNCell);\nexports.LSTMCell = LSTMCell;\n\n_tfjsCore.serialization.SerializationMap.register(LSTMCell);\nvar LSTM = function (_super) {\n    __extends(LSTM, _super);\n    function LSTM(config) {\n        var _this = this;\n        if (config.implementation === 0) {\n            console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n        }\n        config.cell = new LSTMCell(config);\n        _this = _super.call(this, config) || this;\n        return _this;\n    }\n    LSTM.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            var mask = kwargs == null ? null : kwargs['mask'];\n            var training = kwargs == null ? null : kwargs['training'];\n            var initialState = kwargs == null ? null : kwargs['initialState'];\n            return _super.prototype.call.call(_this, inputs, { mask: mask, training: training, initialState: initialState });\n        });\n    };\n    Object.defineProperty(LSTM.prototype, \"units\", {\n        get: function () {\n            return this.cell.units;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"activation\", {\n        get: function () {\n            return this.cell.activation;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"useBias\", {\n        get: function () {\n            return this.cell.useBias;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"kernelInitializer\", {\n        get: function () {\n            return this.cell.kernelInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"recurrentInitializer\", {\n        get: function () {\n            return this.cell.recurrentInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"biasInitializer\", {\n        get: function () {\n            return this.cell.biasInitializer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"unitForgetBias\", {\n        get: function () {\n            return this.cell.unitForgetBias;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"kernelRegularizer\", {\n        get: function () {\n            return this.cell.kernelRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"recurrentRegularizer\", {\n        get: function () {\n            return this.cell.recurrentRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"biasRegularizer\", {\n        get: function () {\n            return this.cell.biasRegularizer;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"kernelConstraint\", {\n        get: function () {\n            return this.cell.kernelConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"recurrentConstraint\", {\n        get: function () {\n            return this.cell.recurrentConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"biasConstraint\", {\n        get: function () {\n            return this.cell.biasConstraint;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"dropout\", {\n        get: function () {\n            return this.cell.dropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"recurrentDropout\", {\n        get: function () {\n            return this.cell.recurrentDropout;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(LSTM.prototype, \"implementation\", {\n        get: function () {\n            return this.cell.implementation;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    LSTM.prototype.getConfig = function () {\n        var config = {\n            units: this.units,\n            activation: (0, _activations.serializeActivation)(this.activation),\n            useBias: this.useBias,\n            kernelInitializer: (0, _initializers.serializeInitializer)(this.kernelInitializer),\n            recurrentInitializer: (0, _initializers.serializeInitializer)(this.recurrentInitializer),\n            biasInitializer: (0, _initializers.serializeInitializer)(this.biasInitializer),\n            unitForgetBias: this.unitForgetBias,\n            kernelRegularizer: (0, _regularizers.serializeRegularizer)(this.kernelRegularizer),\n            recurrentRegularizer: (0, _regularizers.serializeRegularizer)(this.recurrentRegularizer),\n            biasRegularizer: (0, _regularizers.serializeRegularizer)(this.biasRegularizer),\n            activityRegularizer: (0, _regularizers.serializeRegularizer)(this.activityRegularizer),\n            kernelConstraint: (0, _constraints.serializeConstraint)(this.kernelConstraint),\n            recurrentConstraint: (0, _constraints.serializeConstraint)(this.recurrentConstraint),\n            biasConstraint: (0, _constraints.serializeConstraint)(this.biasConstraint),\n            dropout: this.dropout,\n            recurrentDropout: this.recurrentDropout,\n            implementation: this.implementation\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        delete baseConfig['cell'];\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    LSTM.fromConfig = function (cls, config) {\n        if (config['implmentation'] === 0) {\n            config['implementation'] = 1;\n        }\n        return new cls(config);\n    };\n    LSTM.className = 'LSTM';\n    return LSTM;\n}(RNN);\nexports.LSTM = LSTM;\n\n_tfjsCore.serialization.SerializationMap.register(LSTM);\nvar StackedRNNCells = function (_super) {\n    __extends(StackedRNNCells, _super);\n    function StackedRNNCells(config) {\n        var _this = _super.call(this, config) || this;\n        _this.cells = config.cells;\n        return _this;\n    }\n    Object.defineProperty(StackedRNNCells.prototype, \"stateSize\", {\n        get: function () {\n            var stateSize = [];\n            for (var _i = 0, _a = this.cells.slice().reverse(); _i < _a.length; _i++) {\n                var cell = _a[_i];\n                if (Array.isArray(cell.stateSize)) {\n                    stateSize.push.apply(stateSize, cell.stateSize);\n                } else {\n                    stateSize.push(cell.stateSize);\n                }\n            }\n            return stateSize;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    StackedRNNCells.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            inputs = inputs;\n            var states = inputs.slice(1);\n            var nestedStates = [];\n            for (var _i = 0, _a = _this.cells.slice().reverse(); _i < _a.length; _i++) {\n                var cell = _a[_i];\n                if (Array.isArray(cell.stateSize)) {\n                    nestedStates.push(states.splice(0, cell.stateSize.length));\n                } else {\n                    nestedStates.push(states.splice(0, 1));\n                }\n            }\n            nestedStates.reverse();\n            var newNestedStates = [];\n            var callInputs;\n            for (var i = 0; i < _this.cells.length; ++i) {\n                var cell = _this.cells[i];\n                states = nestedStates[i];\n                if (i === 0) {\n                    callInputs = [inputs[0]].concat(states);\n                } else {\n                    callInputs = [callInputs[0]].concat(states);\n                }\n                callInputs = cell.call(callInputs, kwargs);\n                newNestedStates.push(callInputs.slice(1));\n            }\n            states = [];\n            for (var _b = 0, _c = newNestedStates.slice().reverse(); _b < _c.length; _b++) {\n                var cellStates = _c[_b];\n                states.push.apply(states, cellStates);\n            }\n            return [callInputs[0]].concat(states);\n        });\n    };\n    StackedRNNCells.prototype.build = function (inputShape) {\n        if (generic_utils.isArrayOfShapes(inputShape)) {\n            inputShape = inputShape[0];\n        }\n        inputShape = inputShape;\n        var outputDim;\n        for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n            var cell = _a[_i];\n            cell.build(inputShape);\n            if (Array.isArray(cell.stateSize)) {\n                outputDim = cell.stateSize[0];\n            } else {\n                outputDim = cell.stateSize;\n            }\n            inputShape = [inputShape[0], outputDim];\n        }\n        this.built = true;\n    };\n    StackedRNNCells.prototype.getConfig = function () {\n        var cellConfigs = [];\n        for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n            var cell = _a[_i];\n            cellConfigs.push({\n                'className': this.getClassName(),\n                'config': cell.getConfig()\n            });\n        }\n        var config = { 'cells': cellConfigs };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    StackedRNNCells.fromConfig = function (cls, config, customObjects) {\n        if (customObjects === void 0) {\n            customObjects = {};\n        }\n        var cells = [];\n        for (var _i = 0, _a = config['cells']; _i < _a.length; _i++) {\n            var cellConfig = _a[_i];\n            cells.push((0, _serialization.deserialize)(cellConfig, customObjects));\n        }\n        return new cls({ cells: cells });\n    };\n    Object.defineProperty(StackedRNNCells.prototype, \"trainableWeights\", {\n        get: function () {\n            if (!this.trainable) {\n                return [];\n            }\n            var weights = [];\n            for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n                var cell = _a[_i];\n                weights.push.apply(weights, cell.trainableWeights);\n            }\n            return weights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(StackedRNNCells.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            var weights = [];\n            for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n                var cell = _a[_i];\n                weights.push.apply(weights, cell.nonTrainableWeights);\n            }\n            if (!this.trainable) {\n                var trainableWeights = [];\n                for (var _b = 0, _c = this.cells; _b < _c.length; _b++) {\n                    var cell = _c[_b];\n                    trainableWeights.push.apply(trainableWeights, cell.trainableWeights);\n                }\n                return trainableWeights.concat(weights);\n            }\n            return weights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    StackedRNNCells.prototype.getWeights = function () {\n        var weights = [];\n        for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n            var cell = _a[_i];\n            weights.push.apply(weights, cell.weights);\n        }\n        return (0, _variables.batchGetValue)(weights);\n    };\n    StackedRNNCells.prototype.setWeights = function (weights) {\n        var tuples = [];\n        for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n            var cell = _a[_i];\n            var numParams = cell.weights.length;\n            var inputWeights = weights.splice(numParams);\n            for (var i = 0; i < cell.weights.length; ++i) {\n                tuples.push([cell.weights[i], inputWeights[i]]);\n            }\n        }\n        (0, _variables.batchSetValue)(tuples);\n    };\n    StackedRNNCells.className = 'StackedRNNCells';\n    return StackedRNNCells;\n}(RNNCell);\nexports.StackedRNNCells = StackedRNNCells;\n\n_tfjsCore.serialization.SerializationMap.register(StackedRNNCells);\n//# sourceMappingURL=recurrent.js.map"},"hash":"b2622b5a9092cc96f56158cb93a9aead","cacheData":{"env":{}}}