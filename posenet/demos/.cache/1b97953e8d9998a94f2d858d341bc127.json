{"dependencies":[{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1533242702523},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1533242702575},{"name":"babel-runtime/core-js/promise"},{"name":"@tensorflow-models/posenet","loc":{"line":17,"column":25}},{"name":"dat.gui","loc":{"line":18,"column":16}},{"name":"stats.js","loc":{"line":19,"column":18}},{"name":"./demo_util","loc":{"line":20,"column":44}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _promise = require('babel-runtime/core-js/promise');\n\nvar _promise2 = _interopRequireDefault(_promise);\n\nexports.bindPage = bindPage;\n\nvar _posenet = require('@tensorflow-models/posenet');\n\nvar posenet = _interopRequireWildcard(_posenet);\n\nvar _dat = require('dat.gui');\n\nvar _dat2 = _interopRequireDefault(_dat);\n\nvar _stats = require('stats.js');\n\nvar _stats2 = _interopRequireDefault(_stats);\n\nvar _demo_util = require('./demo_util');\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nconst videoWidth = 600;\nconst videoHeight = 500;\nconst stats = new _stats2.default();\n\nfunction isAndroid() {\n  return (/Android/i.test(navigator.userAgent)\n  );\n}\n\nfunction isiOS() {\n  return (/iPhone|iPad|iPod/i.test(navigator.userAgent)\n  );\n}\n\nfunction isMobile() {\n  return isAndroid() || isiOS();\n}\n\n/**\n * Loads a the camera to be used in the demo\n *\n */\nasync function setupCamera() {\n  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {\n    throw new Error('Browser API navigator.mediaDevices.getUserMedia not available');\n  }\n\n  const video = document.getElementById('video');\n  video.width = videoWidth;\n  video.height = videoHeight;\n\n  const mobile = isMobile();\n  const stream = await navigator.mediaDevices.getUserMedia({\n    'audio': false,\n    'video': {\n      facingMode: 'user',\n      width: mobile ? undefined : videoWidth,\n      height: mobile ? undefined : videoHeight\n    }\n  });\n  video.srcObject = stream;\n\n  return new _promise2.default(resolve => {\n    video.onloadedmetadata = () => {\n      resolve(video);\n    };\n  });\n}\n\nasync function loadVideo() {\n  const video = await setupCamera();\n  video.play();\n\n  return video;\n}\n\nconst guiState = {\n  algorithm: 'multi-pose',\n  input: {\n    mobileNetArchitecture: isMobile() ? '0.50' : '0.75',\n    outputStride: 16,\n    imageScaleFactor: 0.5\n  },\n  singlePoseDetection: {\n    minPoseConfidence: 0.1,\n    minPartConfidence: 0.5\n  },\n  multiPoseDetection: {\n    maxPoseDetections: 5,\n    minPoseConfidence: 0.15,\n    minPartConfidence: 0.1,\n    nmsRadius: 30.0\n  },\n  output: {\n    showVideo: false,\n    showSkeleton: true,\n    showPoints: true\n  },\n  net: null\n};\n\ndocument.getElementById('camera-btn').addEventListener('click', () => {\n  if (guiState.output.showVideo) guiState.output.showVideo = false;else guiState.output.showVideo = true;\n});\n\n/**\n * Sets up dat.gui controller on the top-right of the window\n */\nfunction setupGui(cameras, net) {\n  guiState.net = net;\n\n  if (cameras.length > 0) {\n    guiState.camera = cameras[0].deviceId;\n  }\n\n  const gui = new _dat2.default.GUI({ width: 300 });\n\n  // The single-pose algorithm is faster and simpler but requires only one\n  // person to be in the frame or results will be innaccurate. Multi-pose works\n  // for more than 1 person\n  const algorithmController = gui.add(guiState, 'algorithm', ['single-pose', 'multi-pose']);\n\n  // The input parameters have the most effect on accuracy and speed of the\n  // network\n  let input = gui.addFolder('Input');\n  // Architecture: there are a few PoseNet models varying in size and\n  // accuracy. 1.01 is the largest, but will be the slowest. 0.50 is the\n  // fastest, but least accurate.\n  const architectureController = input.add(guiState.input, 'mobileNetArchitecture', ['1.01', '1.00', '0.75', '0.50']);\n  // Output stride:  Internally, this parameter affects the height and width of\n  // the layers in the neural network. The lower the value of the output stride\n  // the higher the accuracy but slower the speed, the higher the value the\n  // faster the speed but lower the accuracy.\n  input.add(guiState.input, 'outputStride', [8, 16, 32]);\n  // Image scale factor: What to scale the image by before feeding it through\n  // the network.\n  input.add(guiState.input, 'imageScaleFactor').min(0.2).max(1.0);\n  input.open();\n\n  // Pose confidence: the overall confidence in the estimation of a person's\n  // pose (i.e. a person detected in a frame)\n  // Min part confidence: the confidence that a particular estimated keypoint\n  // position is accurate (i.e. the elbow's position)\n  let single = gui.addFolder('Single Pose Detection');\n  single.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0);\n  single.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0);\n\n  let multi = gui.addFolder('Multi Pose Detection');\n  multi.add(guiState.multiPoseDetection, 'maxPoseDetections').min(1).max(20).step(1);\n  multi.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0);\n  multi.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0);\n  // nms Radius: controls the minimum distance between poses that are returned\n  // defaults to 20, which is probably fine for most use cases\n  multi.add(guiState.multiPoseDetection, 'nmsRadius').min(0.0).max(40.0);\n  multi.open();\n\n  let output = gui.addFolder('Output');\n  output.add(guiState.output, 'showVideo');\n  output.add(guiState.output, 'showSkeleton');\n  output.add(guiState.output, 'showPoints');\n  output.open();\n\n  architectureController.onChange(function (architecture) {\n    guiState.changeToArchitecture = architecture;\n  });\n\n  algorithmController.onChange(function (value) {\n    switch (guiState.algorithm) {\n      case 'single-pose':\n        multi.close();\n        single.open();\n        break;\n      case 'multi-pose':\n        single.close();\n        multi.open();\n        break;\n    }\n  });\n}\n\n/**\n * Sets up a frames per second panel on the top-left of the window\n */\nfunction setupFPS() {\n  stats.showPanel(0); // 0: fps, 1: ms, 2: mb, 3+: custom\n  document.body.appendChild(stats.dom);\n}\n\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n */\nfunction detectPoseInRealTime(video, net) {\n  const canvas = document.getElementById('output');\n  const ctx = canvas.getContext('2d');\n  // since images are being fed from a webcam\n  const flipHorizontal = true;\n\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n\n  async function poseDetectionFrame() {\n    if (guiState.changeToArchitecture) {\n      // Important to purge variables and free up GPU memory\n      guiState.net.dispose();\n\n      // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or 1.01\n      // version\n      guiState.net = await posenet.load(+guiState.changeToArchitecture);\n\n      guiState.changeToArchitecture = null;\n    }\n\n    // Begin monitoring code for frames per second\n    stats.begin();\n\n    // Scale an image down to a certain factor. Too large of an image will slow\n    // down the GPU\n    const imageScaleFactor = guiState.input.imageScaleFactor;\n    const outputStride = +guiState.input.outputStride;\n\n    let poses = [];\n    let minPoseConfidence;\n    let minPartConfidence;\n    switch (guiState.algorithm) {\n      case 'single-pose':\n        const pose = await guiState.net.estimateSinglePose(video, imageScaleFactor, flipHorizontal, outputStride);\n        poses.push(pose);\n\n        minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;\n        break;\n      case 'multi-pose':\n        poses = await guiState.net.estimateMultiplePoses(video, imageScaleFactor, flipHorizontal, outputStride, guiState.multiPoseDetection.maxPoseDetections, guiState.multiPoseDetection.minPartConfidence, guiState.multiPoseDetection.nmsRadius);\n\n        minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;\n        minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;\n        break;\n    }\n\n    ctx.clearRect(0, 0, videoWidth, videoHeight);\n\n    if (guiState.output.showVideo) {\n      ctx.save();\n      ctx.scale(-1, 1);\n      ctx.translate(-videoWidth, 0);\n      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);\n      ctx.restore();\n    }\n\n    // For each pose (i.e. person) detected in an image, loop through the poses\n    // and draw the resulting skeleton and keypoints if over certain confidence\n    // scores\n    poses.forEach(({ score, keypoints }) => {\n      if (score >= minPoseConfidence) {\n        if (guiState.output.showPoints) {\n          (0, _demo_util.drawKeypoints)(keypoints, minPartConfidence, ctx);\n        }\n        if (guiState.output.showSkeleton) {\n          (0, _demo_util.drawSkeleton)(keypoints, minPartConfidence, ctx);\n        }\n      }\n    });\n\n    // End monitoring code for frames per second\n    stats.end();\n\n    requestAnimationFrame(poseDetectionFrame);\n  }\n\n  poseDetectionFrame();\n}\n\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n */\nasync function bindPage() {\n  // Load the PoseNet model weights with architecture 0.75\n  const net = await posenet.load(0.75);\n\n  document.getElementById('loading').style.display = 'none';\n  document.getElementById('main').style.display = 'block';\n\n  let video;\n\n  try {\n    video = await loadVideo();\n  } catch (e) {\n    let info = document.getElementById('info');\n    info.textContent = 'this browser does not support video capture,' + 'or this device does not have a camera';\n    info.style.display = 'block';\n    throw e;\n  }\n\n  setupGui([], net);\n  setupFPS();\n  detectPoseInRealTime(video, net);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n// kick off the demo\nbindPage();"},"hash":"2de15fbb322cd01b8c6adf37ac8ceeac","cacheData":{"env":{}}}