{"dependencies":[{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/package.json","includedInParent":true,"mtime":1533242702575},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/.babelrc","includedInParent":true,"mtime":1533242702523},{"name":"/Users/Shoshana/Documents/Active/2018/Coding_Bootcamp/SeniorPhase/tfjs-models/posenet/demos/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1533242740549},{"name":"@tensorflow/tfjs-core","loc":{"line":12,"column":36}},{"name":"../backend/tfjs_backend","loc":{"line":13,"column":19}},{"name":"../engine/topology","loc":{"line":14,"column":22}},{"name":"../errors","loc":{"line":15,"column":48}},{"name":"../utils/generic_utils","loc":{"line":16,"column":31}},{"name":"./recurrent","loc":{"line":17,"column":20}},{"name":"./serialization","loc":{"line":18,"column":28}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.Bidirectional = exports.VALID_BIDIRECTIONAL_MERGE_MODES = exports.TimeDistributed = exports.Wrapper = undefined;\nexports.checkBidirectionalMergeMode = checkBidirectionalMergeMode;\n\nvar _tfjsCore = require('@tensorflow/tfjs-core');\n\nvar tfc = _interopRequireWildcard(_tfjsCore);\n\nvar _tfjs_backend = require('../backend/tfjs_backend');\n\nvar K = _interopRequireWildcard(_tfjs_backend);\n\nvar _topology = require('../engine/topology');\n\nvar _errors = require('../errors');\n\nvar _generic_utils = require('../utils/generic_utils');\n\nvar generic_utils = _interopRequireWildcard(_generic_utils);\n\nvar _recurrent = require('./recurrent');\n\nvar _serialization = require('./serialization');\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nvar __extends = undefined && undefined.__extends || function () {\n    var extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (d, b) {\n        d.__proto__ = b;\n    } || function (d, b) {\n        for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() {\n            this.constructor = d;\n        }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n}();\n\nvar Wrapper = function (_super) {\n    __extends(Wrapper, _super);\n    function Wrapper(config) {\n        var _this = _super.call(this, config) || this;\n        _this.layer = config.layer;\n        return _this;\n    }\n    Wrapper.prototype.build = function (inputShape) {\n        this.built = true;\n    };\n    Object.defineProperty(Wrapper.prototype, \"trainable\", {\n        get: function () {\n            if (this.layer != null) {\n                return this.layer.trainable;\n            } else {\n                return false;\n            }\n        },\n        set: function (value) {\n            if (this.layer != null) {\n                this.layer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.layer.trainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            return this.layer.nonTrainableWeights;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"updates\", {\n        get: function () {\n            return this.layer._updates;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Wrapper.prototype, \"losses\", {\n        get: function () {\n            return this.layer.losses;\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Wrapper.prototype.getWeights = function () {\n        return this.layer.getWeights();\n    };\n    Wrapper.prototype.setWeights = function (weights) {\n        this.layer.setWeights(weights);\n    };\n    Wrapper.prototype.getConfig = function () {\n        var config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig()\n            }\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    Wrapper.fromConfig = function (cls, config, customObjects) {\n        if (customObjects === void 0) {\n            customObjects = {};\n        }\n        var layerConfig = config['layer'];\n        var layer = (0, _serialization.deserialize)(layerConfig, customObjects);\n        delete config['layer'];\n        var newConfig = { layer: layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    };\n    return Wrapper;\n}(_topology.Layer);\nexports.Wrapper = Wrapper;\n\nvar TimeDistributed = function (_super) {\n    __extends(TimeDistributed, _super);\n    function TimeDistributed(config) {\n        var _this = _super.call(this, config) || this;\n        _this.supportsMasking = true;\n        return _this;\n    }\n    TimeDistributed.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new _errors.ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + (\"input shape \" + JSON.stringify(inputShape)));\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        _super.prototype.build.call(this, inputShape);\n    };\n    TimeDistributed.prototype.computeOutputShape = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        var childOutputShape = this.layer.computeOutputShape(childInputShape);\n        var timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    };\n    TimeDistributed.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            inputs = generic_utils.getExactlyOneTensor(inputs);\n            var step = function (inputs, states) {\n                var output = _this.layer.call(inputs, kwargs);\n                return [output, []];\n            };\n            var rnnOutputs = (0, _recurrent.rnn)(step, inputs, [], false, null, null, false, inputs.shape[1]);\n            var y = rnnOutputs[1];\n            return y;\n        });\n    };\n    TimeDistributed.className = 'TimeDistributed';\n    return TimeDistributed;\n}(Wrapper);\nexports.TimeDistributed = TimeDistributed;\n\n_tfjsCore.serialization.SerializationMap.register(TimeDistributed);\nvar VALID_BIDIRECTIONAL_MERGE_MODES = exports.VALID_BIDIRECTIONAL_MERGE_MODES = ['sum', 'mul', 'concat', 'ave'];\nfunction checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar Bidirectional = function (_super) {\n    __extends(Bidirectional, _super);\n    function Bidirectional(config) {\n        var _this = _super.call(this, config) || this;\n        var layerConfig = config.layer.getConfig();\n        _this.forwardLayer = (0, _serialization.deserialize)({ className: config.layer.getClassName(), config: layerConfig });\n        layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n        _this.backwardLayer = (0, _serialization.deserialize)({ className: config.layer.getClassName(), config: layerConfig });\n        _this.forwardLayer.name = 'forward_' + _this.forwardLayer.name;\n        _this.backwardLayer.name = 'backward_' + _this.backwardLayer.name;\n        checkBidirectionalMergeMode(config.mergeMode);\n        _this.mergeMode = config.mergeMode;\n        if (config.weights) {\n            throw new _errors.NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        _this._stateful = config.layer.stateful;\n        _this.returnSequences = config.layer.returnSequences;\n        _this.returnState = config.layer.returnState;\n        _this.supportsMasking = true;\n        _this._trainable = true;\n        _this.inputSpec = config.layer.inputSpec;\n        return _this;\n    }\n    Object.defineProperty(Bidirectional.prototype, \"trainable\", {\n        get: function () {\n            return this._trainable;\n        },\n        set: function (value) {\n            this._trainable = value;\n            if (this.forwardLayer != null) {\n                this.forwardLayer.trainable = value;\n            }\n            if (this.backwardLayer != null) {\n                this.backwardLayer.trainable = value;\n            }\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Bidirectional.prototype.getWeights = function () {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    };\n    Bidirectional.prototype.setWeights = function (weights) {\n        var numWeights = weights.length;\n        var numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    };\n    Bidirectional.prototype.computeOutputShape = function (inputShape) {\n        var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        var outputShape;\n        var outputShapes;\n        var stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        } else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        } else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        } else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    };\n    Bidirectional.prototype.apply = function (inputs, kwargs) {\n        var initialState = null;\n        if (kwargs != null) {\n            initialState = kwargs['initialState'];\n        }\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if (initialState == null || initialState.length === 0) {\n            var applyOutputs = _super.prototype.apply.call(this, inputs, kwargs);\n            return applyOutputs;\n        } else {\n            throw new _errors.NotImplementedError('The support for initial states is not implemented for ' + 'Bidirectional layers yet.');\n        }\n    };\n    Bidirectional.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        return (0, _tfjsCore.tidy)(function () {\n            if (kwargs['mask'] != null) {\n                throw new _errors.NotImplementedError('The support for masking is not implemented for ' + 'Bidirectional layers yet.');\n            }\n            if (kwargs['initialState'] != null) {\n                throw new _errors.NotImplementedError('The support for initial states is not implemented for ' + 'Bidirectional layers yet.');\n            }\n            var y = _this.forwardLayer.call(inputs, kwargs);\n            var yRev = _this.backwardLayer.call(inputs, kwargs);\n            var states;\n            if (_this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                } else {}\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (_this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            var output;\n            if (_this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            } else if (_this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            } else if (_this.mergeMode === 'ave') {\n                output = K.scalarTimesArray(K.getScalar(0.5), tfc.add(y, yRev));\n            } else if (_this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            } else if (_this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            if (_this.returnState) {\n                if (_this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    };\n    Bidirectional.prototype.resetStates = function (states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    };\n    Bidirectional.prototype.build = function (inputShape) {\n        var _this = this;\n        K.nameScope(this.forwardLayer.name, function () {\n            _this.forwardLayer.build(inputShape);\n        });\n        K.nameScope(this.backwardLayer.name, function () {\n            _this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    };\n    Object.defineProperty(Bidirectional.prototype, \"trainableWeights\", {\n        get: function () {\n            return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Object.defineProperty(Bidirectional.prototype, \"nonTrainableWeights\", {\n        get: function () {\n            return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n        },\n        enumerable: true,\n        configurable: true\n    });\n    Bidirectional.prototype.getConfig = function () {\n        var config = {\n            'mergeMode': this.mergeMode\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    Bidirectional.fromConfig = function (cls, config) {\n        var rnnLayer = (0, _serialization.deserialize)(config['layer']);\n        delete config['layer'];\n        if (config['numConstants'] != null) {\n            throw new _errors.NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n        }\n        var newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    };\n    Bidirectional.className = 'Bidirectional';\n    return Bidirectional;\n}(Wrapper);\nexports.Bidirectional = Bidirectional;\n\n_tfjsCore.serialization.SerializationMap.register(Bidirectional);\n//# sourceMappingURL=wrappers.js.map"},"hash":"334771435b27716f3b95137704a5a209","cacheData":{"env":{}}}